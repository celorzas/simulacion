#+TITLE: EST-24107: Simulación
#+AUTHOR: Prof. Alfredo Garbuno Iñigo
#+EMAIL:  agarbuno@itam.mx
#+DATE: ~Técnicas de reducción de varianza~
#+STARTUP: showall
:REVEAL_PROPERTIES:
# Template uses org export with export option <R B>
# Alternatives: use with citeproc
#+LANGUAGE: es
#+OPTIONS: num:nil toc:nil timestamp:nil
#+REVEAL_REVEAL_JS_VERSION: 4
#+REVEAL_THEME: night
#+REVEAL_SLIDE_NUMBER: t
#+REVEAL_HEAD_PREAMBLE: <meta name="description" content="Simulación">
#+REVEAL_INIT_OPTIONS: width:1600, height:900, margin:.2
#+REVEAL_EXTRA_CSS: ./mods.css
#+REVEAL_PLUGINS: (notes)
:END:
#+PROPERTY: header-args:R :session varianza :exports both :results output org :tangle ../rscript/04-reduccion-varianza.R :mkdirp yes :dir ../
#+EXCLUDE_TAGS: toc noexport

#+BEGIN_NOTES
*Profesor*: Alfredo Garbuno Iñigo | Primavera, 2022 | Reducción de varianza.\\
*Objetivo*: En esta sección veremos técnicas para reducir la incertidumbre de nuestros estimadores Monte Carlo. Esto lo logramos con dos estrategias: 1) conociendo mas propiedades de nuestro modelo o 2) haciendo un cambio de medida de referencia.\\
*Lectura recomendada*: El contenido de esta sección se puede seguir del Capítulo 5 de citet:Rubinstein. Algunos ejemplos son tomados del Capítulo 5 de citet:Asmussen2007. Las propiedades teóricas, si te interesa, del método por importancia las puedes encontrar en el Capítulo 5 de citet:Rubinstein o echarle un vistazo a la prueba del Teorema 5.6 en las notas de citet:Sanz-Alonso2019. 
#+END_NOTES

#+begin_src R :exports none :results none
  ## Setup --------------------------------------------
  library(tidyverse)
  library(patchwork)
  library(scales)
  ## Cambia el default del tamaño de fuente 
  theme_set(theme_linedraw(base_size = 25))

  ## Cambia el número de decimales para mostrar
  options(digits = 4)
  ## Problemas con mi consola en Emacs
  options(pillar.subtle = FALSE)
  options(rlang_backtrace_on_error = "none")

  sin_lineas <- theme(panel.grid.major = element_blank(),
                      panel.grid.minor = element_blank())
  color.itam  <- c("#00362b","#004a3b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f", NA)

  sin_lineas <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
  sin_leyenda <- theme(legend.position = "none")
  sin_ejes <- theme(axis.ticks = element_blank(), axis.text = element_blank())
#+end_src


* Contenido                                                             :toc:
:PROPERTIES:
:TOC:      :include all  :ignore this :depth 3
:END:
:CONTENTS:
- [[#introducción][Introducción]]
  - [[#error-monte-carlo][Error Monte Carlo]]
- [[#variables-antitéticas][Variables antitéticas]]
  - [[#preguntas][Preguntas:]]
- [[#variables-de-control][Variables de Control]]
  - [[#consideraciones][Consideraciones:]]
  - [[#ejemplo][Ejemplo]]
    - [[#pregunta][Pregunta:]]
- [[#monte-carlo-condicional][Monte Carlo condicional]]
  - [[#ejemplo-mezcla-beta-binomial][Ejemplo: Mezcla Beta-Binomial]]
  - [[#ejemplo-mezcla-poisson-beta][Ejemplo: Mezcla Poisson-Beta]]
  - [[#ejemplo-estimación-de-densidades-tomado-de-citepasmussen2007][Ejemplo: Estimación de densidades (tomado de citep:Asmussen2007)]]
  - [[#ejemplo-constructora-tomado-de-las-notas-de-jorge-de-la-vega][Ejemplo: Constructora (Tomado de las notas de Jorge de la Vega)]]
- [[#muestreo-estratificado][Muestreo estratificado]]
  - [[#diseño-de-experimentos][Diseño de experimentos]]
  - [[#ejemplo][Ejemplo:]]
    - [[#aplicación][Aplicación:]]
  - [[#post-estratificación][Post estratificación]]
- [[#muestreo-por-importancia][Muestreo por importancia]]
- [[#referencia][Referencia]]
:END:

* Introducción

Ya vimos algunos casos donde podemos reducir la varianza de nuestros estimadores
Monte Carlo. Esto nos ayuda a mejorar la velocidad y eficiencia estadística de
nuestros estimadores y en consecuencia optimizar los recursos computacionales.

#+REVEAL: split
Para lograrlo, por ejemplo, consideramos la posibilidad de cambiar la densidad
contra la que estamos realizando el proceso de integración. A esta distribución
le llamaremos ~medida de referencia~.

En esta sección estudiaremos técnicas que nos permitirán reducir la varianza de
nuestros estimadores.

** Error Monte Carlo

Lo que estamos tratando de resolver es el problema de
\begin{align}
\theta = \mathbb{E}_\pi(h(X))\,,
\end{align}
por medio de
1. Generar  muestras $X_{1}, \ldots, X_{N} \overset{\mathsf{iid}}{\sim} \pi$.
2. Estimar por medio de $\hat \theta_N = (1/N) \sum_{i = 1}^{N} h(X_i)$.

#+REVEAL: split
Bajo ciertas condiciones, un intervalo de confianza ($1-\delta$) puede construirse por medio
\begin{align}
[\hat \theta_N - z_{1-\delta/2} \, \mathsf{ee}(\hat \theta_N), \hat \theta_N + z_{1-\delta/2} \, \mathsf{ee}(\hat \theta_N)]\,,
\end{align}
donde podemos calcular el error estándar del estimador (~Error Monte Carlo~).

#+REVEAL: split
Hemos jugado con la noción de medir la calidad de nuestro estimador Monte
Carlo al observar la longitud del intervalo. Es por esto que utilizamos la
~longitud media~ (~HW~) del intervalo de confianza
\begin{align}
\mathsf{HW}= z_{1-\delta/2} \, \mathsf{ee}(\hat \theta_N)\,.
\end{align}
 
#+REVEAL: split
Veremos técnicas de reducción de varianza que nos ayudarán a reducir la longitud media.

#+BEGIN_NOTES
El uso de técnicas de reducción de varianza nos obligan a conocer un poco más
sobre el modelo que está detrás de nuestro estimador. Esto es, para mejorar
nuestra estimación $\hat \pi_N^{\mathsf{MC}}(f)$ tenemos que conocer más
propiedades tanto de $f$ y/o $\pi$. Pues esto nos ayudará a reducir aún mas
nuestra varianza.
#+END_NOTES

\newpage

* Variables antitéticas

- Lo que buscaremos es inducir una correlación negativa entre secuencias de números pseudo-aleatorios.
- La idea es que al generar números en pares una observación grande en la primera secuencia se compense con una observación pequeña en la segunda.
- El ejemplo típico es sincronizar  $u_n \sim \mathsf{U}(0,1)$ con $u_n' = 1 - u_n$.

#+REVEAL: split
Supongamos que queremos estimar
\begin{align}
\pi(h) = \mathbb{E}[h(X)] = \mathbb{E}[Y]\,.
\end{align}
Supongamos que tenemos dos muestras $Y_1$ y $Y_2$. El estimador
\begin{align}
\hat \pi(h) = (Y_1 + Y_2)/2\,,
\end{align}
tiene
\begin{align}
\mathbb{V}(\hat \pi(h)) = \frac{\mathbb{V}(Y_1) + \mathbb{V}(Y_2) + 2 \mathsf{Cov}(Y_1, Y_2)}{4}\,.
\end{align}

*** ~Preguntas~:
:PROPERTIES:
:reveal_background: #00468b
:END:
- Si $Y_1$  y $Y_2$ son ~iid~. ¿Cuál es el valor de $\mathbb{V}(\hat \pi (h))$?
- ¿Cómo reducimos la varianza? 

** Intuición

Supongamos que tenemos $(X^{(1)}_{1}, \ldots, X^{(1)}_{N})$ y $(X^{(2)}_{1}, \ldots, X^{(2)}_{N})$ en donde,
para generar $X^{(1)}_j$, se utilizó $u_j$ y para generar $X^{(2)}_j$ se utilizó $1 - u_j$. Suponemos que tenemos $X_j^{(i)} \overset{\mathsf{iid}}{\sim} \mathbb{P}$. 

*** ~Preguntas~:
:PROPERTIES:
:reveal_background: #00468b
:END:
1. ¿Cuál es el valor esperado de $X^{(1)}_j$ y $X^{(2)}_j$?
2. ¿Cuál es el signo de $\mathsf{Cov}(X^{(1)}_j, X^{(2)}_j)$?
3. ¿Son independientes?
4. ¿Qué pasa con los pares $(X^{(1)}_j, X^{(2)}_j)$ y $(X^{(1)}_k, X^{(2)}_k)$?



*** ~Conclusiones~:
Por lo anterior, si definimos
\begin{align}
X_j = \frac{X^{(1)}_j + X^{(2)}_j}{2}\,, \qquad \bar X_N = \frac1N \sum_{n = 1}^{N} X_n\,,
\end{align}
tenemos un estimador que tiene la siguientes propiedades:
1. Es insesgado.
2. Tiene menor varianza que una muestra de $2N$ simulaciones.


*** ~Consideraciones~:
No siempre se puede lograr el objetivo. Es decir, depende del modelo.


** Fundamento

Queremos estimar
\begin{align}
\pi(h) = \mathbb{E}[h(U)]\,,
\end{align}
donde $U \sim \mathsf{U}(0,1)$.

#+REVEAL: split
- Si suponemos que $h$ es ~no decreciente~. Entonces, si $U$ es grande también $h(U)$ será grande. Al mismo tiempo $1-U$ y $h(1-U)$ serán pequeños. Esto implica que el término $\mathsf{Cov}(h(U), h(1-U)) < 0$.


#+REVEAL: split
- Si suponemos que $h$ es ~no creciente~. Entonces, podemos concluir también que el término  $\mathsf{Cov}(h(U), h(1-U)) < 0$.


#+REVEAL: split
- Entonces, una condición suficiente para garantizar que se reduce la varianza es por medio de $h$ una función ~monótona~.

*** ~Teorema~:
Si $h(u_{1}, \ldots, u_{m})$ es una función monótona en cada uno de sus argumentos en $[0,1]^m$, entonces para una colección de variables aleatorias $U_i \overset{\mathsf{iid}}{\sim} \mathsf{U}(0,1)$ tenemos que
\begin{align}
\mathsf{Cov}\left(  h(U_{1}, \ldots, U_{m}), h(1-U_{1}, \ldots, 1-U_{m})\right) < 0 \,.
\end{align}


*** ~Extensión~:
Podemos extender el resultado anterior bajo el método de la transformada inversa. Es decir, podemos definir
\begin{align}
h(\mathbb{P}^{-1}_{1}(U_1), \ldots, \mathbb{P}^{-1}_{m}(U_m))\,.
\end{align}

#+BEGIN_NOTES
Por definición la función de acumulación es no decreciente. Y por lo tanto, las inversas también son no decrecientes. 
#+END_NOTES

** Ejemplo: Integral en intervalo

Queremos estimar $\int_{a}^{b} f(x) \text{d}x$. El estimador Monte Carlo sería
\begin{align}
\hat \pi_N^{\mathsf{MC}}(f) = \frac{b-a}{N} \sum_{n = 1}^{N} f(x_n)\,,
\end{align}
donde $x_n \sim \mathsf{U}(a, b)$.

#+REVEAL: split
Si escogemos la mitad (aleatoriamente) y por cada muestra usamos $x'_n = a + (b - x_n)$.
Entonces tendríamos 
\begin{align}
\hat \pi_N^{\mathsf{AMC}}(f) = \frac{b-a}{N/2} \sum_{n = 1}^{N/2} \frac{f(x_n) + f(x'_n)}{2}\,,
\end{align}

#+REVEAL: split
#+begin_src R :exports code :results none
  set.seed(108)
  nsamples <- 10^3;
  a <- 2; b <- 3;
  u <- runif(nsamples, min = a, max = b)
  x <- dnorm(u)
#+end_src

#+begin_src R :exports results :results org 
  c(estimador = mean(x), error.std = sd(x)/sqrt(nsamples), N = length(x))
#+end_src

#+RESULTS:
#+begin_src org
estimador error.std         N 
2.153e-02 1.396e-02 1.000e+03
#+end_src

#+begin_src R :exports code :results none 
  u_ <- a + (b - u)
  x_ <- dnorm(u_)
  x  <- (x + x_)/2
  ax <- x[1:(nsamples/2)]
#+end_src

#+begin_src R :exports results :results org 
  c(estimador = mean(ax), error.std = sd(ax)/sqrt(nsamples), N = length(ax))
#+end_src

#+RESULTS:
#+begin_src org
estimador error.std         N 
2.133e-02 3.518e-03 5.000e+02
#+end_src


* Variables de Control

Supongamos que queremos estimar $\mathbb{E}(X)$ y tenemos acceso a una variable aleatoria $Y$ que está ~correlacionada~ y se conoce $\nu = \mathbb{E}(Y)$. A $Y$ se le conoce como ~variable control~ de $X$.

#+REVEAL: split
Sea $X_c = X - a ( Y - \nu)$. Entonces
1. $\mathbb{E}(X_c) = \mathbb{E}(X)$.
2. $\mathbb{V}(X_c) = \mathbb{V}(X - a ( Y - \nu)) = \mathbb{V}(X) + a^2 \mathbb{V}(Y) - 2 a \mathsf{Cov}(X,Y)$. Esto implica que
   \begin{align}
   \mathbb{V}(X_c) \leq \mathbb{V}(X)\, \quad \text{ si }  \quad 2 a \mathsf{Cov } (X,Y) > a^2 \mathbb{V}(Y)\,.
   \end{align}
3. El caso particular
   \begin{align}
   a^* = \frac{\mathsf{Cov}(X,Y)}{\mathbb{V}(Y)}\,,
   \end{align}
   que induce la mínima varianza.
4. En este último caso
   \begin{align}
   \mathbb{V}(X_c) = (1 - \rho^2_{X,Y}) \mathbb{V}(X)\,.
   \end{align}


** Consideraciones:
En la práctica no siempre se conoce el valor de $\mathbb{V}(Y)$ y muy difícilmente la $\mathsf{Cov}(X,Y)$, lo que implica que es difícil conocer el valor de $a$. 

#+REVEAL: split
En la práctica se puede utilizar un estudio piloto para estimar $a$ citep:Lavenberg1982. Esto es,
\begin{align}
\hat a_M = \frac{\widehat{\mathsf{Cov}}_M(X,Y)}{\widehat{\mathbb{V}}_M(Y)}\,.
\end{align}
Nota que el estimador resultante para la media de $X_c$ ya no es un estimador insesgado.

** Ejemplo

Supongamos que $X \sim \mathsf{N}(0,1)$ y que $f(X)= \frac{X^6}{1 + X^2}$.

- Entonces, utilizando la igualdad
  \begin{align}
  \frac{x^6}{1 + x^2} = x^4 - x^2 + 1 - \frac{1}{1 + x^2}\,,
  \end{align}
  y podemos aproximar con $Y = g(X)= x^4 - x^2 + 1$.
- Para esta elección tenemos $\mathbb{E}(Y) = 3$.
- Asi que el problema se reduce a
  \begin{align}
  \mathbb{E} \left[  \frac{X^6}{1 + X^2}\right] = 3 - \mathbb{E} \left[ \frac{1}{1 + X^2}\right]\,.
  \end{align}


#+REVEAL: split
#+begin_src R :exports code :results none 
  set.seed(108)
  x <- rnorm(nsamples)
#+end_src

#+begin_src R :exports both :results org 
  f_x <- x**6/(1 + x**2)
  c(estimador = mean(f_x), error.std = sd(f_x)/sqrt(nsamples))
#+end_src

#+RESULTS:
#+begin_src org
estimador error.std 
   2.3473    0.2798
#+end_src

#+begin_src R :exports both :results org 
  g_x <- 3 - 1 / (1 + x**2)
  c(estimador = mean(g_x), error.std = sd(g_x)/sqrt(nsamples) )
#+end_src

#+RESULTS:
#+begin_src org
estimador error.std 
 2.343346  0.008549
#+end_src

*** ~Pregunta~:
:PROPERTIES:
:reveal_background: #00468b
:END:
¿Por qué estos estimadores dan los mismas números que con el código anterior? 

#+begin_src R :exports both :results org
  set.seed(108)
  x <- rnorm(100 * nsamples)
  x <- array(x, c(100, nsamples))
  f_x <- x**6/(1 + x**2)
  estimadores <- apply(f_x, 1, mean)
  c(estimador = mean(estimadores), error.std = sd(estimadores))
#+end_src

#+RESULTS:
#+begin_src org
estimador error.std 
   2.3473    0.2752
#+end_src

#+begin_src R :exports both :results org 
  g_x <- 3 - 1/(1+x**2)
  estimadores <- apply(g_x, 1, mean)
  c(estimador = mean(estimadores), error.std = sd(estimadores))
#+end_src

#+RESULTS:
#+begin_src org
estimador error.std 
   2.3453    0.0081
#+end_src

* Monte Carlo condicional

Se pueden utilizar algunos resultados teóricos intermedios para algunos casos. A esta técnica también se le conoce como método ~Rao-Blackwell~ (por un resultado análogo en inferencia estadística). 

#+REVEAL: split
Supongamos que nos interesa $\mathbb{E}(f(X))$ y del alguna manera tenemos conocimiento de una variable aleatoria que está relacionada con la original por medio de $\mathbb{E}(f(X) |Z = z)$. Utilizando la propiedad torre podemos calcular
\begin{align}
\mathbb{E}(f(X)) = \mathbb{E}\left( \mathbb{E}(f(X) | Z = z) \right) \,.
\end{align}

Donde además tenemos que
\begin{align}
\mathbb{V}(f(X)) = \mathbb{V}(E(f(X)|Z)) + \mathbb{E}(\mathbb{V}(f(X)|Z))\,.
\end{align}

#+REVEAL: split
Lo que buscamos es que:
1. $Z$ pueda ser generado de manera eficiente.
2. Se pueda calcular $\mathbb{E}(f(X)|Z)$.
3. El valor de $\mathbb{E}(\mathbb{V}(f(X)|Z))$ sea grande. 

#+REVEAL: split
Por lo tanto, el método es:
1. Generar una muestra $Z_{1}, \ldots, Z_{N} \overset{\mathsf{iid}}{\sim} \pi(Z)$ .
2. Calcular $\mathbb{E}(f(X)| Z = z_k)$ de manera analítica.
3. Calcular el estimador de $\pi(f) = \mathbb{E}(f(X))$ por medio de
   \begin{align}
   \hat \pi_N^{\mathsf{CMC}} (f) = \frac1N \sum_{n = 1}^{N} \mathbb{E}(f(X)| Z = Z_k)\,.
   \end{align}
   


** Ejemplo: Mezcla Beta-Binomial

Supongamos un modelo Beta-Binomial. Igual que antes asumamos $n = 20$ y $\alpha = 2, \beta = 5$.

#+begin_src R :exports both :results org 
  set.seed(108)
  theta <- rbeta(nsamples, 2, 5)
  y <- rbinom(nsamples, size = 20, theta)
  c(estimador = mean(y), error.std = sd(y)/sqrt(nsamples))
#+end_src

#+RESULTS:
#+begin_src org
estimador error.std 
    5.585     0.119
#+end_src

#+REVEAL: split
#+begin_src R :exports both :results org 
  m_y <- 20 * theta
  c(estimador = mean(m_y), error.std = sd(m_y)/sqrt(nsamples))
#+end_src

#+RESULTS:
#+begin_src org
estimador error.std 
    5.587     0.102
#+end_src

#+REVEAL: split
El porcentaje de reducción de varianza es
#+begin_src R :exports results :results org 
  (sd(y) - sd(m_y))/sd(y)
#+end_src


** Ejemplo: Mezcla Poisson-Beta

Supongamos un modelo de mezcla
#+begin_src R :exports both :results org 
  set.seed(108)
  w <- rpois(nsamples, 10)
  y <- rbeta(nsamples, w, w**2 + 1)
  c(estimador = mean(y), error.std = sd(y)/sqrt(nsamples))
#+end_src

#+RESULTS:
#+begin_src org
estimador error.std 
 0.096535  0.001404
#+end_src

#+REVEAL: split
#+begin_src R :exports both :results org 
  m_y <- w / (w**2 + w + 1)
  c(estimador = mean(m_y), error.std = sd(m_y)/sqrt(nsamples))
#+end_src

#+RESULTS:
#+begin_src org
estimador error.std 
 0.098341  0.001019
#+end_src

#+REVEAL: split
El porcentaje de reducción de varianza es
#+begin_src R :exports results :results org 
  (sd(y) - sd(m_y))/sd(y)
#+end_src

#+RESULTS:
#+begin_src org
[1] 0.2737
#+end_src


** Ejemplo: Estimación de densidades (tomado de citep:Asmussen2007)

Podemos utilizar el método Monte Carlo condicionado para estimar densidades. Por ejemplo, si consideramos que $X_{1}, \ldots, X_{k} \overset{\mathsf{iid}}{\sim} \pi$ y nos interesa $S_k = X_{1} + \cdots + X_{k}$. Nos podemos preguntar por al densidad de la suma. Sabemos que la densidad es un objeto infinitesimal $\mathbb{P}(S_k \in \text{d}x)$. Y en algunas situaciones no tenemos acceso a éste.

#+REVEAL: split
Por ejemplo, consideremos $X_i \sim \mathsf{Pareto}(1, \alpha = 3/2)$. Para este caso, no se puede escribir la densidad de $S_k$ para $k > 1$. Lo que si sabemos es que
\begin{align}
S_k \, | \, S_{k-1} \overset{\mathsf{d}}{=} X_k \, |\, S_{k-1} \sim \mathsf{Pareto}(S_{k-1}, \alpha)\,.
\end{align}
Por lo que podemos estimar la densidad de $X_k \,|\, S_{k-1}$ para valores, por ejemplo, en $[0, 15)$.


#+REVEAL: split
#+begin_src R :exports code :results none
  nsamples <- 5 * 10^3; ngrid <- 1000; k <- 4
  rpareto <- function(n, alpha) { 1 / runif(n)^(1/alpha) - 1 }
  dpareto <- function(x, alpha) {
    ifelse( x >= 0, (alpha / ((x+1)**(alpha + 1))), 0) }
  u <- rpareto( (k-1) * nsamples, alpha = 3/2)
  u <- array(u, c(k-1, nsamples))
  S <- apply(u, 2, sum)
  x <- seq(0.1, 15, length.out = ngrid)
#+end_src

#+REVEAL: split
#+begin_src R :exports code :results none 
  estimador <- array(x, c(ngrid,1)) |>
    apply(1, FUN = function(x_){ dpareto(x_ - S, alpha = 3/2) }) |>
    apply(2, mean)

  error.std <- array(x, c(ngrid,1)) |>
    apply(1, FUN = function(x_){ dpareto(x_ - S, alpha = 3/2) }) |>
    apply(2, sd)
#+end_src

#+begin_src R :exports none :results none
  k <- 8
  u <- rpareto( (k-1) * nsamples, alpha = 3/2)
  u <- array(u, c(k-1, nsamples))
  S <- apply(u, 2, sum)

  estimador.8 <- array(x, c(ngrid,1)) |>
    apply(1, FUN = function(x_){ dpareto(x_ - S, alpha = 3/2) }) |>
    apply(2, mean)

  error.std.8 <- array(x, c(ngrid,1)) |>
    apply(1, FUN = function(x_){ dpareto(x_ - S, alpha = 3/2) }) |>
    apply(2, sd)
#+end_src

#+REVEAL: split
#+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/pareto-density-estimate.jpeg :exports results :results output graphics file
  g1 <- tibble(x, estimador, error.std) |>
  ggplot(aes(x, estimador)) +
    geom_ribbon(aes(ymin = estimador - 2 * error.std/sqrt(nsamples),
                    ymax = estimador + 2 * error.std/sqrt(nsamples)),
                fill = 'salmon', alpha = .3) + 
    geom_line() + sin_lineas + ggtitle(expression(k==4))

  g2 <- tibble(x, estimador = estimador.8, error.std = error.std.8) |>
  ggplot(aes(x, estimador)) +
    geom_ribbon(aes(ymin = estimador - 2 * error.std/sqrt(nsamples),
                    ymax = estimador + 2 * error.std/sqrt(nsamples)),
                fill = 'salmon', alpha = .3) + 
    geom_line() + sin_lineas + ggtitle(expression(k==8))

  g1 + g2
#+end_src
#+caption: Densidad de $x \,|\, S_{k-1}$. 
#+RESULTS:
[[file:../images/pareto-density-estimate.jpeg]]


** Ejemplo: Constructora (Tomado de las notas de Jorge de la Vega)

Un proyecto de construcción tiene una duración $X \sim \mathsf{N}(\mu, \sigma^2)$ donde, a su vez, $\mu \sim \mathsf{N}(10, 16)$ y $\sigma \sim \mathsf{Exp}(1/4)$. La compañía que construye debe pagar $1,000$ (USD) por cada día (y prorratea por fracciones del día) que la duración del proyecto excede el contrato de $K$ días. ¿Cuál es el costo esperado por retraso?

#+REVEAL: split
Podemos simular
#+begin_src R :exports code :results none 
  K <- 20; nsamples <- 10^4
  sigma <- rexp(nsamples, 1/4)
  mu    <- rnorm(nsamples, mean = 10, sd = 4)
  x     <- rnorm(nsamples, mean = mu, sd = sigma)
  costo <- 1000 * ifelse( x <= K, 0, x - K)
#+end_src

#+begin_src R :exports results :results org 
  c(media = mean(costo), error.std = sd(costo)/sqrt(nsamples))
#+end_src

#+RESULTS:
#+begin_src org
    media error.std 
   316.49     21.26
#+end_src

#+REVEAL: split
Con condicionales, sabemos que podemos considerar $\theta = (\mu, \sigma)$ y evaluar $X | \theta$. Lo que nos lleva a escribir que nuestro estimador será sobre
\begin{align*}
\mathbb{E}_{X|\theta} \left[ 1000 \max \{X - K, 0\} \right] &= 1000 \int_{K}^{\infty} \frac{X - K}{\sqrt{2\pi \sigma^2}} \exp \left[  -\frac12 \left( \frac{x - \mu }{\sigma} \right)^2\right] \text{d}x\\
&= 1000 \int_{K'}^{\infty} (\sigma \nu + \mu - K) \frac{1}{\sqrt{2\pi}} \exp \left[ -\frac12 \nu^2\right] \text{d}\nu \\
&= 1000 \left[\left( - \frac{\sigma e^{-\frac{\nu^2}{2}}}{\sqrt{2\pi}} \right) \bigg|^{\infty}_{K'} + (\mu - K) \Phi \left( -K' \right) \right]\\
&= 1000 \left[\sigma \phi(K') + (\mu - K) \Phi \left( -K' \right)\right] \,.\\
\end{align*}


#+REVEAL: split
La estimación utilizando Monte Carlo condicional nos da una estimación (con su métrica de error Monte Carlo). 
#+begin_src R :exports results :results org 
  costo.cond <- 1000 * (sigma * dnorm((K - mu)/sigma) - (K - mu) * pnorm( (mu - K)/sigma ))
  c(media = mean(costo.cond), error.std = sd(costo.cond)/sqrt(nsamples))
#+end_src

#+RESULTS:
#+begin_src org
    media error.std 
  301.920     8.301
#+end_src

Lo que lleva a una reducción de varianza
#+begin_src R :exports results :results org 
  (sd(costo) - sd(costo.cond))/sd(costo)
#+end_src

#+RESULTS:
#+begin_src org
[1] 0.5645
#+end_src



* Muestreo estratificado 

Queremos estimar $\mathbb{E}_\pi[h(X)]$ y supongamos que existe una variable
aleatoria discreta $Y$ con soporte $y_1, \ldots, y_k$ tal que
1. Las probabilidades $\omega_i = \mathsf{Prob}\{Y = y_i\}$ son conocidas;
2. Para cada $i$ podemos simular de la condicional $\pi_i(X) = \pi(X | Y = y_i)$.


#+REVEAL: split
Si queremos usar simulación para estimar $\pi(h)$ entonces utilizaríamos una
muestra aleatoria $h(X_{1}), \ldots, h(X_{N})$ y utilizaríamos su promedio para estimarlo.
La varianza de este estimador sería igual a
\begin{align}
\mathbb{V}(\hat \pi^{\mathsf{MC}}_N(h)) = \frac{\mathbb{V}_\pi(h)}{N}\,.
\end{align}

#+REVEAL: split
Ahora, si realizamos $N_i = N \times \omega_i$ simulaciones para cada nivel $i$,
y promediamos para cada nivel tendríamos
\begin{align}
\hat \pi^{\mathsf{MC}}_{N,i}(h) = \frac{1}{N_i} \sum_{n = 1}^{N_i} h(X_n^{(i)})\,, \qquad X_n^{(i)} \overset{\mathsf{iid}}{\sim} \pi_i(X)\,,
\end{align}
de tal forma que podemos construir el estimador
\begin{align}
\hat \pi^{\mathsf{sMC}}_N(h) = \sum_{i = 1}^{k} \omega_i \, \hat \pi^{\mathsf{MC}}_{N, i}(h)\,.
\end{align}

#+REVEAL: split
La varianza de cada término es igual a
\begin{align}
\mathbb{V}(\hat \pi^{\mathsf{MC}}_{N,i}(h)) = \frac{\mathbb{V}(h(X) | Y = y_i)}{N_i}\,.
\end{align}
Por lo tanto la varianza de nuestro estimador es
\begin{align}
\mathbb{V}(\hat \pi^{\mathsf{sMC}}_{N}(h)) &= \sum_{i = 1}^{k} \omega_i^2 \, \mathbb{V}(\hat \pi^{\mathsf{MC}}_{N,i}(h)) \\
&= \frac1N \sum_{i = 1}^{k} \omega_i \, \mathbb{V}(h(X) | Y = y_i)\\
&= \frac1N \mathbb{E}[\mathbb{V}(h(X)|Y)]\,.
\end{align}


#+REVEAL: split
Utilizando lo que sabemos de Monte Carlo condicional sabemos que tendremos una
ganancia de
\begin{align}
\mathbb{V}(\hat \pi^{\mathsf{MC}}_{N}(h)) - \mathbb{V}(\hat \pi^{\mathsf{sMC}}_{N}(h)) = \frac1N \mathbb{V}\left(\mathbb{E}(h(X) |Y)\right)\,.
\end{align}

La ganancia será mayor mientras más afecte el valor de $Y$ el valor esperado de $h(X)$.


** Diseño de experimentos

Notemos que el estimador estratificado tiene una varianza igual a 
\begin{align}
\mathbb{V}(\hat \pi^{\mathsf{sMC}}_{N}(h)) =   \sum_{i = 1}^{k} \frac{\omega_i^2}{N_i} \, \mathbb{V}(h(X) | Y = y_i)\,.
\end{align}
Lo cual asume que conocemos los términos individuales.

#+REVEAL: split
Sin embargo, usualmente no conoceremos $\mathbb{V}(h(X) | Y = y_i)$ lo que nos
lleva a que podríamos usar un pequeño piloto de simulación para poder
estimarlos. Denotaremos por $s_i^2$ dichos estimadores.

#+REVEAL: split
Si sabemos que tenemos un presupuesto de $N$ simulaciones y queremos distribuir nuestras simulaciones entre la partición. Entonces podemos resolver el problema de
\begin{gather*}
\min \sum_{i = 1}^{k} \omega_i^2 s_i^2 / N_i \\
\text{sujeto a } \sum_{i = 1}^{k} N_i = N \,.
\end{gather*}

El cual tiene una solución 
\begin{align}
\frac{N_i^\star}{N} = \frac{\omega_i s_i}{\sum_{j = 1}^{k} \omega_j s_j}\,.
\end{align}

** Ejemplo:

Supongamos que queremos resolver la integral
\begin{align}
\pi(h) = \int_{0}^{1} h(x) \text{d}x\,.
\end{align}
#+REVEAL: split
Si definimos
\begin{align}
Y = j \quad \text{ si } \frac{j - 1}{N} \leq U < \frac{j}{N}\,, \quad j = 1, \ldots, N\,,
\end{align}
entonces podemos calcular
\begin{align}
\pi(h) &= \frac1N \sum_{j = 1}^{N} \mathbb{E}[h(U) | Y = j]\\
&= \frac1N \sum_{j = 1}^{N} \mathbb{E}[h(U^{(j)})]\,,
\end{align}
donde $U^{(j)} \sim \mathsf{U}((j-1)/N, j/N)$. 

#+REVEAL: split
Por lo que en lugar de generar $U_{1}, \ldots, U_{N} \sim \mathsf{U}(0,1)$ para
calcular $\sum_j h(U_j)/N$ , podemos construir un mejor estimador por medio de
\begin{align}
\hat \pi^{sMC}_N(h) = \frac1N \sum_{j = 1}^{N} h \left( \frac{{U_j + j - 1}}{N} \right)\,.
\end{align}

*** ~Aplicación~:
Estimemos $\pi$ por medio de
\begin{align}
\frac\pi4 = \mathbb{E}[\sqrt{1 - U^2}]\,.
\end{align}

#+begin_src R :exports code :results none 
  nsamples <- 5000
  h <- function(u) { 4 * sqrt(1 - u**2) }
  u <- runif(100 * nsamples)
  u <- array(u, c(100, nsamples))
  h_u <- h(u)
  estimador_MC <- apply(h_u, 1, cummean) |> t()
#+end_src

#+REVEAL: split
 #+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/pi-vainillia.jpeg :exports results :results output graphics file
  as_tibble(t(estimador_MC[1:50,])) |>
    mutate(n = 1:nsamples) |>
    pivot_longer(cols = 1:50) |>
    ggplot(aes(n, value, group = name)) +
    geom_line(aes(color = name), alpha = .8) +
    geom_hline(yintercept = pi, lty = 2) + 
    scale_x_continuous(trans='log10', 
                       labels = trans_format("log10", math_format(10^.x))) + 
    ylab('Aproximación') + xlab("Número de muestras") + sin_lineas + sin_leyenda
#+end_src

#+RESULTS:
[[file:../images/pi-vainillia.jpeg]]

#+REVEAL: split
#+begin_src R :exports code :results none 
  runif_estrat <- function(u){
    x <- c()
    for (jj in 1:nsamples){
      x[jj] <- (u[jj] + jj - 1)/nsamples 
    }
    return(x)
  }
  u_strat <- apply(u, 1, runif_estrat) |> t()
#+end_src

#+begin_src R :exports none :results none
  h_strat <- h(u_strat)
  estimador_sMC <- apply(h_strat, 1, mean)
#+end_src

#+REVEAL: split
#+begin_src R :exports code :results none 
  calcula_antitetic <- function(u){
    x <- c()
    for (jj in 1:nsamples){
      x[jj] <- h((u[jj] + jj - 1)/nsamples) + h((jj - u[jj])/nsamples)
    }
    return(0.5 * x)
  }
  h_anti <- apply(u, 1, calcula_antitetic) |> t()
#+end_src

#+begin_src R :exports none :results none
  estimador_asMC <- apply(h_anti, 1, mean)
#+end_src

#+REVEAL: split
#+begin_src R :exports results :results org 
  options(digits = 7)
  tibble(metodo = c('vainilla', 'estratificado', 'anti-estratificado'),
         estimador = c( apply(h_u, 1, mean) |> mean(),
                        apply(h_strat, 1, mean) |> mean(),
                        apply(h_anti, 1, mean) |> mean()),
         error.mc = c( apply(h_u, 1, mean) |> sd(),
                      apply(h_strat, 1, mean) |> sd(),
                      apply(h_anti, 1, mean) |> sd())
         ) |> as.data.frame()
#+end_src

#+RESULTS:
#+begin_src org
              metodo estimador     error.mc
1           vainilla  3.141487 1.400903e-02
2      estratificado  3.141592 7.065134e-06
3 anti-estratificado  3.141593 6.824089e-07
#+end_src


** Post estratificación

Supongamos que hemos simulado $N$ réplicas independientes de una variable
aleatoria $X\sim \mathbb{P}$. Supongamos que podemos identificar los grupos, es
decir, podemos decir cuando una simulación está asociada a una $Y$ con categoría
$y_i$ de tal forma que con $N_i$ contamos cuántas simulaciones pertenecen a la
categoría $i$. Entonces, podríamos escribir
\begin{align*}
\bar X_N &= \frac1N \sum_{n = 1}^{N} X_n\\
&= \frac1N \sum_{i = 1 }^{k} N_i \bar X_{N_i}^{(i)}\\
&= \sum_{i = 1}^{k} \frac{N_i}{N} \bar X_{N_i}^{(i)}\,.
\end{align*}

* Muestreo por importancia                                      

Supongamos que queremos estimar
\begin{align}
\pi(h) = \int_{}^{} h(x) \pi(x) \text{d}x\,.
\end{align}
Sin embargo, consideremos que evaluar $h$ es ineficiente debido a:
1. Es difícil simular un $\text{valor}^\dagger$ aleatorio de la densidad $\pi$.
2. La varianza de $h$  es muy grande.
3. Una combinación de 2. y 3.

#+REVEAL: split
Podemos utilizar una distribución $\rho$ tal que para $\rho(x) = 0$  tenemos $\pi(x) = 0$. Entonces, podemos reescribir
\begin{align}
\pi(h) &= \int \frac{h(x) \pi(x)}{\rho(x)} \rho(x) \text{d}x \\
&= \mathbb{E}_\rho \left[ \frac{h(x) \cdot \pi(x) }{\rho(x)} \right] \\
&= \rho( h \omega )\,,
\end{align}
donde
\begin{align}
\omega(x) = \frac{\pi(x)}{\rho(x)}\,.
\end{align}

#+REVEAL: split
En aplicaciones usualmente operamos bajo el supuesto que conocemos los pesos ~hasta una constante de normalización~. Esto es, podemos ~evaluar~
\begin{align}
\omega(x) = \frac{\pi(x)}{\rho(x)} = \frac{1}{Z} v(x)\,.
\end{align}
donde $Z = \int v(x) \rho(x) \text{d}x = \rho(v)$.
Lo cual nos deja
\begin{align}
\pi(h) = \frac{\rho(h v)}{\rho(v)}\,.
\end{align}

#+REVEAL: split
~Muestreo por importancia~ se basa en aproximar ambas integrales por el método Monte Carlo. Es decir, utilizamos
\begin{align}
\pi(h) &\approx \sum_{n = 1}^{N} \omega_n h(x_n)\, \qquad x_n \overset{\mathsf{iid}}{\sim} \rho\\
&= \hat \pi_N^{\mathsf{IS}}(h)\,,
\end{align}
donde
\begin{align}
\omega_n = \frac{v(x_n)}{\sum_{m = 1}^{N} v(x_m)}\,.
\end{align}


#+REVEAL: split
La construcción de nuestro estimador de esta manera tiene algunas propiedades interesantes. El estimador $\hat \pi^{\mathsf{IS}}_N(h)$  *no* es un estimador insesgado (aunque asintóticamente si). 



#+BEGIN_NOTES
El estimador que hemos construido asume que conocemos los pesos /hasta/ una constante de normalización. En aplicaciones esto es usual, pues la densidad $\pi$ que define nuestros problemas de integración suele ser muy complicada. Es por esto que la discusión en esta sección lo ha tratado de esta manera. 
#+END_NOTES




#+REVEAL: split
Para que muestreo por importancia tenga éxito necesitamos que $h(x) \pi(x)/\rho(x)$ tenga una varianza pequeña.

#+REVEAL: split
En general, la elección de la función de muestreo, $\rho$, está asociada a la varianza del estimador $\pi^{\mathsf{IS}}_N(h)$ y por lo tanto la elección es crucial para que tenga éxito.

#+REVEAL: split
El problema de escoger la mejor $\rho^\star$ es que tendríamos que escoger dependiendo del problema (la elección de $h$ y $\pi$) y esto no presenta estrategias muy aplicables en la práctica. 

* Referencia

bibliographystyle:abbrvnat
bibliography:references.bib


* Plan                                                             :noexport:

Propiedades 5.7.2. 

#+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/ac-function.jpeg :exports results :results output graphics file
rerun(5, rnorm(100)) %>%
  set_names(paste0("sim", 1:5)) %>%
  map(~ accumulate(., ~ .05 + .x + .y)) %>%
  map_dfr(~ tibble(value = .x, step = 1:100), .id = "simulation") %>%
  ggplot(aes(x = step, y = value)) +
    geom_line(aes(color = simulation)) +
    ggtitle("Simulations of a random walk with drift")
#+end_src

#+RESULTS:
[[file:../images/ac-function.jpeg]]
